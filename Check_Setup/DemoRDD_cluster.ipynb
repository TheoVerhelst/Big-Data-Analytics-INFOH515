{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD example\n",
    "\n",
    "This notebook provide a dummy example of a map on Spark RDD, that can be used to check that parallelisation works fine on the cluster.\n",
    "\n",
    "It consist of creating an RDD with `n_partitions` partitions, and apply a map function that waits for 2 seconds for each partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "A Spark session is created by using the pyspark.sql.SparkSession object. See [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sparksession) for the API documentation on the SparkSession Object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g\\\n",
    "                                    pyspark-shell\"\n",
    "\n",
    "# For Yarn, so that Spark knows where it runs\n",
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "# For Yarn, so Spark knows which version to use (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python3\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below to recreate a Spark session with other parameters\n",
    "#spark.stop()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",\"5\") \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start dummy Spark jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait function\n",
    "def wait2s(x):\n",
    "    time.sleep(2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions=8\n",
    "\n",
    "data=range(0,n_partitions)\n",
    "datardd=sc.parallelize(data,n_partitions)\n",
    "\n",
    "datardd.map(wait2s).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Spark UI and check parallelisation\n",
    "\n",
    "* Open Hadoop Web UI at `127.0.0.1:8088`, and click on your running Application. You should land on an URL similar to `http://127.0.0.1:8088/cluster/app/application_1523870291186_0032`\n",
    "* Change `cluster/app` to `proxy` to get to the Spark UI: `http://127.0.0.1:8088/proxy/application_1523870291186_003`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ten runs (rows), for number of executors in (1,2,5,10,20,50,100) (in columns)\n",
    "n_executors=[1,2,5,10,20,50,100]\n",
    "results_benchmark=np.zeros((10,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(n_executors)):\n",
    "    \n",
    "    print(\"Run benchmark with \"+str(n_executors[i])+\" executors\")\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",str(n_executors[i])) \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    sc=spark.sparkContext\n",
    "    \n",
    "    #100 partitions\n",
    "    n_partitions=100\n",
    "    data=range(0,n_partitions)\n",
    "    datardd=sc.parallelize(data,n_partitions)\n",
    "\n",
    "    for j in range(10):\n",
    "        time_start=time.time()\n",
    "        datardd.map(wait2s).collect()\n",
    "        time_end=time.time()\n",
    "        execution_time=time_end-time_start\n",
    "        results_benchmark[j,i]=execution_time\n",
    "    \n",
    "    spark.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results=pd.DataFrame(results_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results.to_csv(\"resultsBenchmark.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(N,n,random_seed):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    np.random.seed(0)   \n",
    "\n",
    "    #Inputs and the weights of the linear combination are drawn at random\n",
    "    X=np.random.rand(N,n)\n",
    "    theta=np.random.rand(n)\n",
    "    #noise=np.random.rand(N)\n",
    "\n",
    "    Y=np.dot(X,theta)#+noise\n",
    "    Y=Y[:,np.newaxis]\n",
    "    Z=np.concatenate((X,Y),axis=1)\n",
    "\n",
    "    print(\"Number of observations :\",N)\n",
    "    print(\"Number of features :\",n)\n",
    "\n",
    "    print(\"Dimension of X :\",X.shape)\n",
    "    print(\"Dimension of theta :\",theta.shape)\n",
    "    print(\"Dimension of Y :\",Y.shape)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time to create artificial data: \",round(end - start,2),\"seconds\")\n",
    "    \n",
    "    return (X,Y,Z,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us generate the dataset 10M rows, 100 features\n",
    "N=1000000\n",
    "n=100\n",
    "(X,Y,Z,theta)=genData(N,n,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=20g\\\n",
    "                                    pyspark-shell\"\n",
    "\n",
    "# For Yarn, so that Spark knows where it runs\n",
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "# For Yarn, so Spark knows which version to use (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "os.environ['PYSPARK_PYTHON']=\"/etc/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/etc/anaconda3/bin/python\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtx_xty_row(z):\n",
    "    x=np.array(z[:-1])\n",
    "    y=z[-1]\n",
    "    xtx=np.outer(x,x)\n",
    "    xty=np.dot(x,y)\n",
    "    return (xtx,xty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ten runs (rows), for number of executors in (1,2,5,10,20,50,100) (in columns)\n",
    "n_executors=[1,2,5,10,20,50,100]\n",
    "results_benchmark=np.zeros((10,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [6,5,4,3,2,1,0]:#range(len(n_executors)):\n",
    "    \n",
    "    #mem_per_exec=np.min(np.round(20/n_executors[i]),2)\n",
    "    mem_per_exec=str(max([int(np.round(10/n_executors[i])),2]))+\"g\"\n",
    "    \n",
    "    print(\"Number of executors :\"+str(n_executors[i]))\n",
    "    print(\"Memory per executor: \"+str(mem_per_exec))\n",
    "    \n",
    "    #spark.stop()\n",
    "    print(\"Run benchmark with \"+str(n_executors[i])+\" executors\")\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",n_executors[i]) \\\n",
    "    .config(\"spark.executor.memory\",mem_per_exec) \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    sc=spark.sparkContext\n",
    "    \n",
    "    time_start=time.time()\n",
    "    \n",
    "    B=400\n",
    "    Z_RDD=sc.parallelize(Z,B)#.cache()\n",
    "    \n",
    "    time_end=time.time()\n",
    "    \n",
    "    print(\"Time to load data: \"+str(time_end-time_start)+\" s\")\n",
    "    \n",
    "    print(Z_RDD.count())\n",
    "    \n",
    "    for j in range(10):\n",
    "        time_start=time.time()\n",
    "        \n",
    "        (XtX,XtY)=Z_RDD.map(xtx_xty_row)\\\n",
    "        .reduce(lambda xtx_xty0,xtx_xty1:(xtx_xty0[0]+xtx_xty1[0],xtx_xty0[1]+xtx_xty1[1]))\n",
    "\n",
    "        XtX_inverse=np.linalg.inv(XtX)\n",
    "\n",
    "        theta_hat=np.dot(XtX_inverse,XtY)\n",
    "\n",
    "        time_end=time.time()\n",
    "        \n",
    "        execution_time=time_end-time_start\n",
    "        results_benchmark[j,i]=execution_time\n",
    "    \n",
    "    spark.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results=pd.DataFrame(results_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results.to_csv(\"resultsBenchmarkRegression.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
